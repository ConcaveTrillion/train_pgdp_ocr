{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ca365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# import os\n",
    "import subprocess\n",
    "\n",
    "from doctr.datasets.vocabs import VOCABS\n",
    "\n",
    "vocab = VOCABS[\"multilingual\"] + \"⸺¡¿—‘’“”′″\" + VOCABS[\"currency\"]\n",
    "\n",
    "\n",
    "def finetune_detection(train_path, val_path, model_output_path, n_epochs=100):\n",
    "    DOCTR_BASE = \"../doctr\"\n",
    "    SCRIPT_PATH = \"references/detection/train_pytorch.py\"\n",
    "    ARGS = [\n",
    "        \"--train_path\",\n",
    "        train_path,\n",
    "        \"--val_path\",\n",
    "        val_path,\n",
    "        \"db_resnet50\",\n",
    "        \"--pretrained\",\n",
    "        \"--lr\",\n",
    "        \".002\",\n",
    "        \"--epochs\",\n",
    "        str(n_epochs),\n",
    "        \"--batch_size\",\n",
    "        str(2),\n",
    "        \"--device\",\n",
    "        \"0\",\n",
    "        \"--name\",\n",
    "        model_output_path,\n",
    "    ]\n",
    "\n",
    "    command = [\"python\", pathlib.Path(DOCTR_BASE, SCRIPT_PATH).resolve(), *ARGS]\n",
    "\n",
    "    subprocess.call(command)\n",
    "\n",
    "\n",
    "def finetune_recognition(train_path, val_path, name, n_epochs=100):\n",
    "    DOCTR_BASE = \"../doctr\"\n",
    "    SCRIPT_PATH = \"references/recognition/train_pytorch.py\"\n",
    "    ARGS = [\n",
    "        \"--train_path\",\n",
    "        train_path,\n",
    "        \"--val_path\",\n",
    "        val_path,\n",
    "        \"crnn_vgg16_bn\",\n",
    "        \"--pretrained\",\n",
    "        \"--lr\",\n",
    "        \".001\",\n",
    "        \"--epochs\",\n",
    "        str(n_epochs),\n",
    "        \"--name\",\n",
    "        name,\n",
    "        \"--device\",\n",
    "        \"0\",\n",
    "        \"--vocab\",\n",
    "        \"CUSTOM:\" + vocab,\n",
    "    ]  # this does not include em dashes, upside down question marks, etc. TODO - define a vocab to use in training that includes these\n",
    "\n",
    "    command = [\"python\", pathlib.Path(DOCTR_BASE, SCRIPT_PATH).resolve(), *ARGS]\n",
    "\n",
    "    subprocess.call(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctr_base = \"doctr_package\"\n",
    "\n",
    "detection_train_path = \"ml-training/detection\"\n",
    "detection_val_path = \"ml-validation/detection\"\n",
    "recognition_train_path = \"ml-training/recognition\"\n",
    "recognition_val_path = \"ml-validation/recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_detection(\n",
    "    detection_train_path, detection_val_path, \"ml-models/detection-model-finetuned\"\n",
    ")\n",
    "\n",
    "finetune_recognition(\n",
    "    recognition_train_path,\n",
    "    recognition_val_path,\n",
    "    \"ml-models/recognition-model-finetuned\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a184258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW is not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from doctr.models import ocr_predictor, detection_predictor, recognition_predictor\n",
    "# from doctr.models import db_resnet50, crnn_vgg16_bn\n",
    "# from importlib import reload\n",
    "# import doctr.models\n",
    "# from torch import load as torch_load\n",
    "\n",
    "# from doctr_package.doctr.models.recognition.crnn.pytorch import CRNN\n",
    "\n",
    "# reload(doctr.models)\n",
    "\n",
    "\n",
    "# def create_doctr_model(det_pt_path=None, reco_pt_path=None):\n",
    "#     det_model = db_resnet50(pretrained=True).to(\"cuda\")\n",
    "\n",
    "#     if det_pt_path is not None:\n",
    "#         det_params = torch_load(det_pt_path, map_location=\"cuda:0\")\n",
    "#         det_model.load_state_dict(det_params)\n",
    "\n",
    "#     if reco_pt_path is not None:\n",
    "#         reco_model: CRNN = crnn_vgg16_bn(\n",
    "#             pretrained=True,\n",
    "#             pretrained_backbone=True,\n",
    "#             vocab=vocab,\n",
    "#         ).to(\"cuda\")\n",
    "#         reco_params = torch_load(reco_pt_path, map_location=\"cuda:0\")\n",
    "#         reco_model.load_state_dict(reco_params)\n",
    "#     else:\n",
    "#         reco_model = crnn_vgg16_bn(\n",
    "#             pretrained=True,\n",
    "#             pretrained_backbone=True,\n",
    "#         ).to(\"cuda\")\n",
    "\n",
    "#     full_predictor = ocr_predictor(\n",
    "#         det_arch=det_model,\n",
    "#         reco_arch=reco_model,\n",
    "#         pretrained=True,\n",
    "#         assume_straight_pages=True,\n",
    "#         disable_crop_orientation=True,\n",
    "#     )\n",
    "\n",
    "#     det_predictor = detection_predictor(\n",
    "#         arch=det_model,\n",
    "#         pretrained=True,\n",
    "#         assume_straight_pages=True,\n",
    "#     )\n",
    "\n",
    "#     reco_predictor = recognition_predictor(\n",
    "#         arch=reco_model,\n",
    "#         pretrained=True,\n",
    "#     )\n",
    "\n",
    "#     # baseline performs optimally with default\n",
    "#     # the finetuned model seems to work much better when instantiated this way\n",
    "#     # the ocr_predictor must have some extra default config setting\n",
    "#     # that hinders the finetuned model\n",
    "#     if det_pt_path is not None:\n",
    "#         full_predictor.det_predictor = det_predictor\n",
    "#     if reco_pt_path is not None:\n",
    "#         full_predictor.reco_predictor = reco_predictor\n",
    "\n",
    "#     # this might tighten up boxes a bit\n",
    "#     # full_predictor.det_predictor.model.postprocessor.unclip_ratio = 1.2\n",
    "#     # det_predictor.model.postprocessor.unclip_ratio = 1.2\n",
    "\n",
    "#     return det_predictor, full_predictor\n",
    "\n",
    "\n",
    "# baseline_det_predictor, baseline_ocr_predictor = create_doctr_model(None)\n",
    "# finetuned_det_predictor, finetuned_ocr_predictor = create_doctr_model(\n",
    "#     \"training-output/detection-model-finetuned.pt\",\n",
    "#     \"training-output/recognition-model-finetuned.pt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e54ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from doctr_package.doctr.io.reader import DocumentFile\n",
    "# from os import listdir\n",
    "\n",
    "# # this is a bit of a hack, but it works\n",
    "# train_finetune_preds = []\n",
    "# train_baseline_preds = []\n",
    "# train_finetune_exports = []\n",
    "# train_baseline_exports = []\n",
    "# for f in listdir(detection_train_path + \"/images\"):\n",
    "#     print(f)\n",
    "#     doctr_image = DocumentFile.from_images(detection_train_path + \"/images/\" + f)\n",
    "\n",
    "#     baseline_result = baseline_det_predictor(doctr_image)\n",
    "#     train_baseline_preds.append(baseline_result[0][\"words\"])\n",
    "\n",
    "#     baseline_export = baseline_ocr_predictor(doctr_image).export()\n",
    "#     train_baseline_exports.append(baseline_export)\n",
    "\n",
    "#     finetuned_result = finetuned_det_predictor(doctr_image)\n",
    "#     train_finetune_preds.append(finetuned_result[0][\"words\"])\n",
    "\n",
    "#     finetuned_export = finetuned_ocr_predictor(doctr_image).export()\n",
    "#     train_finetune_exports.append(finetuned_export)\n",
    "\n",
    "# val_finetune_preds = []\n",
    "# val_baseline_preds = []\n",
    "# val_finetune_exports = []\n",
    "# val_baseline_exports = []\n",
    "# for f in listdir(detection_val_path + \"/images\"):\n",
    "#     print(f)\n",
    "#     doctr_image = DocumentFile.from_images(detection_val_path + \"/images/\" + f)\n",
    "#     # print(doctr_image)\n",
    "\n",
    "#     finetuned_result = finetuned_ocr_predictor.det_predictor(doctr_image)\n",
    "#     val_finetune_preds.append(finetuned_result[0][\"words\"])\n",
    "\n",
    "#     finetuned_export = finetuned_ocr_predictor(doctr_image).export()\n",
    "#     val_finetune_exports.append(finetuned_export)\n",
    "\n",
    "#     baseline_result = baseline_ocr_predictor.det_predictor(doctr_image)\n",
    "#     val_baseline_preds.append(baseline_result[0][\"words\"])\n",
    "\n",
    "#     baseline_export = baseline_ocr_predictor(doctr_image).export()\n",
    "#     train_baseline_exports.append(baseline_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ce965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO this needs to be update for the targets to check\n",
    "\n",
    "# from doctr.utils.metrics import LocalizationConfusion\n",
    "\n",
    "# lc_baseline_train = LocalizationConfusion(iou_thresh=0.5)\n",
    "# lc_baseline_val = LocalizationConfusion(iou_thresh=0.5)\n",
    "# lc_finetune_train = LocalizationConfusion(iou_thresh=0.5)\n",
    "# lc_finetune_val = LocalizationConfusion(iou_thresh=0.5)\n",
    "\n",
    "# print(\"format: recall, precision, mean_iou\")\n",
    "# print(\"TRAIN\")\n",
    "# for i, fname in enumerate(listdir(detection_train_path + \"/images\")):\n",
    "#     print(\"========\")\n",
    "#     print(fname)\n",
    "#     lc_baseline_for_file = LocalizationConfusion(iou_thresh=0.5)\n",
    "#     lc_finetune_for_file = LocalizationConfusion(iou_thresh=0.5)\n",
    "\n",
    "#     target = targets[i]\n",
    "#     baseline_pred = train_baseline_preds[i]\n",
    "#     finetune_pred = train_finetune_preds[i]\n",
    "\n",
    "#     lc_baseline_for_file.update(target[\"words\"], baseline_pred[:, :4])\n",
    "#     lc_finetune_for_file.update(target[\"words\"], finetune_pred[:, :4])\n",
    "#     print(\"baseline:\", lc_baseline_for_file.summary())\n",
    "#     print(\"finetune:\", lc_finetune_for_file.summary())\n",
    "#     lc_baseline_train.update(target[\"words\"], baseline_pred[:, :4])\n",
    "#     lc_finetune_train.update(target[\"words\"], finetune_pred[:, :4])\n",
    "\n",
    "# print(\"\\nVAL\")\n",
    "# for i, fname in enumerate(val_fnames):\n",
    "#     print(\"========\")\n",
    "#     print(fname)\n",
    "#     lc_baseline_for_file = LocalizationConfusion(iou_thresh=0.5)\n",
    "#     lc_finetune_for_file = LocalizationConfusion(iou_thresh=0.5)\n",
    "\n",
    "#     target = val_targets[i]\n",
    "#     baseline_pred = val_baseline_preds[i]\n",
    "#     finetune_pred = val_finetune_preds[i]\n",
    "\n",
    "#     lc_baseline_for_file.update(target[\"words\"], baseline_pred[:, :4])\n",
    "#     lc_finetune_for_file.update(target[\"words\"], finetune_pred[:, :4])\n",
    "#     print(\"baseline:\", lc_baseline_for_file.summary())\n",
    "#     print(\"finetune:\", lc_finetune_for_file.summary())\n",
    "#     lc_baseline_val.update(target[\"words\"], baseline_pred[:, :4])\n",
    "#     lc_finetune_val.update(target[\"words\"], finetune_pred[:, :4])\n",
    "\n",
    "\n",
    "# print(\"\\noverall baseline (train):\", lc_baseline_train.summary())\n",
    "# print(\"overall finetune (train):\", lc_finetune_train.summary())\n",
    "# print(\"overall baseline (val):\", lc_baseline_val.summary())\n",
    "# print(\"overall finetune (val):\", lc_finetune_val.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO this is broken\n",
    "# doctr_image = DocumentFile.from_images(\n",
    "#     detection_train_path + \"/images/\" + train_fnames[4]\n",
    "# )\n",
    "\n",
    "# baseline_ocr_predictor(doctr_image).show()\n",
    "# finetuned_ocr_predictor(doctr_image).show()\n",
    "\n",
    "# # TODO - add comparison of the baseline vs finetuned recongition models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
